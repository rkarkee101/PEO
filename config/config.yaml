# Example config. Copy into your own project and edit.

project:
  name: process-engineering-optimizer

storage:
  root: ./storage

logging:
  level: INFO

data:
  # Measurement CSV format
  delimiter: ","
  encoding: "utf-8"

  # Columns in your measurement CSV
  tool_parameters: [temperature_C, pressure_mTorr, power_W, flow_sccm, gas]
  target_properties: [thickness_nm, sheet_resistance_ohm]

  # Optional: specify categorical columns and their levels. If levels are empty,
  # they are inferred from the CSV.
  categorical_params:
    gas: []

  # Optional: column units (only for documentation and report text)
  units:
    temperature_C: C
    pressure_mTorr: mTorr
    power_W: W
    flow_sccm: sccm
    thickness_nm: nm
    sheet_resistance_ohm: ohm

# DOE planning
# Each method writes a DOE plan CSV under storage/runs/<run>/doe/
# Supported: full_factorial, fractional_factorial, plackett_burman,
# latin_hypercube, central_composite, box_behnken

doe:
  methods: [full_factorial, fractional_factorial, plackett_burman, latin_hypercube]
  n_samples: 24
  fractional_resolution: 3
  interaction_depth: 2

# DOE -> ML feature engineering (optional)
# If enabled, the pipeline uses DOE analysis (p-values) to:
#  1) select a smaller set of influential factors, and
#  2) append interaction features for significant numeric-numeric pairs.
# It then trains additional model variants with a "+doe" suffix (e.g., gp+doe).
doe_to_ml:
  enabled: false
  selection:
    p_value_threshold: 0.15
    top_k: 8
    keep_at_least: 3
    always_keep: []
  interactions:
    enabled: true
    p_value_threshold: 0.15
    top_k: 10
  # Optional curvature terms (x^2) for numeric factors.
  # This makes the DOE->ML feature space closer to a classic DOE response surface.
  quadratic:
    enabled: true
    p_value_threshold: 0.20
    top_k: 6

# Training and model comparison
training:
  # Models to consider.
  # - gp: Gaussian process (good uncertainty)
  # - random_forest: strong baseline, uncertainty via ensemble dispersion
  # - mlp: neural ensemble (can work well but needs overfit guard)
  # - rsm: DOE-native response surface (Bayesian ridge)
  # - rsm_gp_residual: response surface mean + GP residual (best for DOE-sized data)
  # - rsm_mlp: DOE-informed neural hybrid (Bayesian RSM trend + MLP residual ensemble)
  models: [gp, random_forest, mlp]
  autotune: true
  max_tuning_trials: 35
  tuning_timeout_s: 0
  # Tuning backend:
  # - auto: use Optuna if installed, otherwise use internal random search (default)
  # - optuna: require Optuna
  # - random: internal random search (no extra deps)
  tuning:
    method: auto
  test_size: 0.2
  cv_folds: 5
  random_state: 42

  # Basic overfit guard: a large train/test R2 gap sets overfit_flag=true
  overfit_guard:
    max_train_test_r2_gap: 0.15

  # CV-first model selection (recommended for DOE-sized datasets).
  model_selection:
    metric: cv_rmse          # cv_rmse | test_rmse
    overfit_penalty: 0.25    # adds penalty when train/test R2 gap exceeds overfit_guard
    # Automatic overfit protection (recommended for DOE-sized data):
    # If the top-ranked model is flagged as overfit, PEO will optionally
    # choose a non-overfit model if it is close in score.
    prefer_non_overfit: true
    enforce_non_overfit: false
    non_overfit_score_tolerance: 0.05

  # Uncertainty calibration (std scaling) based on CV residuals.
  uncertainty_calibration:
    enabled: true
    method: quantile
    target_coverage: 0.6827  # ~1-sigma for Normal
    min_std: 1.0e-9
    min_scale: 0.25
    max_scale: 25.0

  split:
    stratify_by: null        # optional categorical column name

# Inverse design (target -> proposed tool settings)
inverse_design:
  # method: random (fast, robust) or bayesopt (requires optional extra: pip install -e ".[bo]")
  method: random
  search_budget: 6000
  top_k: 10
  lambda_uncertainty: 0.35
  # Optional out-of-distribution (OOD) penalty discourages recipes far from training data.
  lambda_ood: 0.0
  # Non-uniqueness handler: return diverse recipes, not near-duplicates.
  diversity:
    enabled: true
    min_distance: 0.12
    max_candidates: 500

  # Multi-target inverse design (used by `peo query-multi` and multi-target chat prompts).
  multi_target:
    enabled: true
    weights: {}     # optional: {target_name: weight}
    tolerances: {}  # optional: {target_name: tolerance}

# RAG indexing
rag:
  retriever: tfidf              # tfidf or sentence_transformers
  st_model: all-MiniLM-L6-v2    # only used if retriever=sentence_transformers
  top_k: 6

# Multi-objective Bayesian optimization (EHVI-style) via Ax/BoTorch
# Requires optional deps: pip install -e ".[mobo]"
# When enabled, the pipeline auto-exports suggested recipes under:
#   storage/runs/<run>/reports/suggested_recipes_mobo_<run_id>.csv
mobo:
  enabled: false
  n_suggestions: 12
  objectives:
    - name: thickness_nm
      goal: maximize
    - name: sheet_resistance_ohm
      goal: minimize
  # Optional constraints, e.g. keep resistance below a threshold:
  # outcome_constraints:
  #   - "sheet_resistance_ohm <= 12"
  outcome_constraints: []
